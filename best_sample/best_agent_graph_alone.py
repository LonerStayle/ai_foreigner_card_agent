"""
Agent Patterns â€” Constrained ReAct vs FSM (Production-Style Demo)

âœ… ëª©ì 
- í˜„ì—…ì‹ êµ¬ì¡°ë¡œ "Constrained ReAct + Graph" ì™€ "ìˆœìˆ˜ FSM(Graph-only)" ë‘ ê°€ì§€ íŒ¨í„´ì„ í•œ íŒŒì¼ì—ì„œ ë¹„êµ ì‹¤í–‰í•  ìˆ˜ ìˆê²Œ ì˜ˆì œ ì œê³µí•©ë‹ˆë‹¤.
- LangGraphë¥¼ ì“°ëŠ” ë²„ì „(í”„ë¡œë•ì…˜ ì§€í–¥)ê³¼, ì˜ì¡´ì„± ì¤„ì¸ ìˆœìˆ˜ íŒŒì´ì¬ FSM ë²„ì „ ë‘˜ ë‹¤ í¬í•¨.

ğŸ§° ì˜ì¡´ì„± (ì„ íƒ)
- langgraph>=0.2
- langchain-openai>=0.2
- python-dotenv (ì„ íƒ)

ì„¤ì¹˜ ì˜ˆ:
  pip install "langgraph>=0.2" "langchain-openai>=0.2" python-dotenv
í™˜ê²½ë³€ìˆ˜:
  export OPENAI_API_KEY=...

ì‹¤í–‰ ì˜ˆ:
  uv run python best_sample/best_agent_graph_alone.py --mode react   # Constrained ReAct + Graph
  uv run python best_sample/best_agent_graph_alone.py --mode fsm     # ìˆœìˆ˜ FSM(Graph-only)

ë©”ì‹œì§€ íë¦„
- Constrained ReAct: ë…¸ë“œ ë‚´ë¶€ì—ì„œ ë‹¨ì¼/ì†Œìˆ˜ ìŠ¤í…ì˜ Reasonâ†’Actâ†’Obsë¥¼ í—ˆìš©(íˆ´ì€ enum ì œí•œ), ìƒìœ„ Graphê°€ ë¶„ê¸°/ì¤‘ë‹¨/ê°€ë“œë ˆì¼ ì œì–´
- FSM: ì „ ë‹¨ê³„ ê²°ì •ì  ì‹¤í–‰(ê²€ì¦â†’ì •ê·œí™”â†’APIâ†’í…œí”Œë¦¿), LLMì€ ë¬¸ì¥ ë‹¤ë“¬ê¸° ì •ë„ë¡œë§Œ ì‚¬ìš©
"""

from __future__ import annotations
from dataclasses import dataclass
from typing import Literal, List, Dict, Any, Optional
from enum import Enum
import argparse
import os
import time

# =====( ì„ íƒ: LangGraph / LangChain )=====
try:
    from langgraph.graph import StateGraph, END
    from typing_extensions import TypedDict
    _LG = True
except Exception:
    # LangGraph ë¯¸ì„¤ì¹˜ í™˜ê²½ì—ì„œë„ íŒŒì¼ì´ importëŠ” ë˜ë„ë¡
    _LG = False
    class TypedDict(dict):
        pass
    END = "__END__"

try:
    from langchain_openai import ChatOpenAI
    _LC = True
except Exception:
    _LC = False

# =====( ê³µí†µ: ë„êµ¬ ì •ì˜ )=====
class Tool(Enum):
    SEARCH = "search"
    DB = "db"
    WEATHER = "weather"

# Mock ë„êµ¬ êµ¬í˜„ (ì‹¤ë¬´ì—ì„  ì‹¤ì œ API/DB ì—°ë™)
def tool_search(query: str) -> Dict[str, Any]:
    time.sleep(0.05)
    return {"tool": Tool.SEARCH.value, "items": [
        {"title": "ì„œìš¸ ë‚ ì”¨ ê°€ì´ë“œ", "source": "docs/weather_guide.md"},
        {"title": f"í‚¤ì›Œë“œ('{query}') ê´€ë ¨ ë¬¸ì„œ", "source": "kb/related.md"},
    ]}

def tool_db(sql: str) -> Dict[str, Any]:
    time.sleep(0.05)
    return {"tool": Tool.DB.value, "rows": [{"id": 1, "name": "í™ê¸¸ë™"}]}

def tool_weather(city: str) -> Dict[str, Any]:
    time.sleep(0.05)
    # ì‹¤ì œì—ì„  ì™¸ë¶€ API í˜¸ì¶œ
    return {"tool": Tool.WEATHER.value, "city": city, "temp_c": 29, "status": "Sunny"}

ALLOWED_TOOLS = {
    Tool.SEARCH.value: tool_search,
    Tool.DB.value: tool_db,
    Tool.WEATHER.value: tool_weather,
}

# =====( ê³µí†µ: LLM ì„ íƒ â€” ì‹¤ë¬´ëŠ” OpenAI, ë°ëª¨ëŠ” Mock )=====
class LLM:
    def __init__(self, model: Optional[str] = None, temperature: float = 0.0):
        self.model = model or "gpt-4o-mini"
        self.temperature = temperature
        self._impl = None
        if _LC and os.getenv("OPENAI_API_KEY"):
            self._impl = ChatOpenAI(model=self.model, temperature=self.temperature)

    def choose_tool(self, question: str, context_snippets: List[str]) -> str:
        """ì œì•½ëœ íˆ´ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒ. í”„ë¡œë•ì…˜ì—ì„œëŠ” í•¨ìˆ˜ì½œ/ìŠ¤í‚¤ë§ˆë¡œ ê°•ì œ.
        ì—¬ê¸°ì„  ê°„ë‹¨ íœ´ë¦¬ìŠ¤í‹± + (ê°€ëŠ¥í•˜ë©´) LLM ë°±ì—….
        """
        # ìš°ì„  ê°„ë‹¨ íœ´ë¦¬ìŠ¤í‹±
        q = question.lower()
        if any(k in q for k in ["ë‚ ì”¨", "weather", "ê¸°ì˜¨", "ë¯¸ì„¸ë¨¼ì§€"]):
            return Tool.WEATHER.value
        if any(k in q for k in ["sql", "db", "ê³ ê°", "íšŒì›"]):
            return Tool.DB.value
        # ê¸°ë³¸ê°’
        guess = Tool.SEARCH.value

        # ê°€ëŠ¥í•˜ë©´ LLMì— ì¬í™•ì¸(íˆ´ ì´ë¦„ë§Œ ì¶”ì¶œ ìœ ë„)
        if self._impl:
            prompt = f"""
            You are a tool selector. Choose one tool name from this list: [search, db, weather].
            Question: {question}
            Context: {context_snippets[:3]}
            Answer with ONLY one of: search | db | weather
            """
            try:
                out = self._impl.invoke(prompt)
                text = (out.content or "").strip().lower()
                if text in ALLOWED_TOOLS:
                    return text
            except Exception:
                pass
        return guess

    def polish(self, text: str) -> str:
        if self._impl:
            try:
                out = self._impl.invoke(f"Polish this text concisely in Korean: {text}")
                return out.content
            except Exception:
                pass
        return text  # mock fallback

# =====( 1) Constrained ReAct + Graph )=====
class ReActState(TypedDict):
    question: str
    route: Literal["rag", "tool", "direct"]
    ctx: List[Dict[str, Any]]
    answer: str
    citations: List[str]
    tries: int
    grounded: float
    trace: List[str]

MAX_STEPS = 2  # ReAct-lite: ë‚´ë¶€ ë‹¨ê³„ ì œí•œ

# --- Router (ê°„ë‹¨ íœ´ë¦¬ìŠ¤í‹±) ---
def router(state: ReActState) -> ReActState:
    q = state["question"].lower()
    if any(k in q for k in ["ê·¼ê±°", "ì¶œì²˜", "ì–´ë””", "ì–¸ì œ", "ë¬´ì—‡", "ê²€ìƒ‰", "docs", "kb"]):
        state["route"] = "rag"
    elif any(k in q for k in ["ë‚ ì”¨", "weather", "db", "sql", "ê³ ê°", "íšŒì›"]):
        state["route"] = "tool"
    else:
        state["route"] = "direct"
    state["trace"].append(f"Routerâ†’{state['route']}")
    return state

# --- RAG ì¤€ë¹„ (ë°ëª¨: ê°„ë‹¨ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸) ---
def retrieve_ctx(question: str) -> List[Dict[str, Any]]:
    res = tool_search(question)
    return [{"text": i["title"], "source": i["source"], "score": 0.7} for i in res["items"]]

# --- ReAct-lite ë…¸ë“œ: 1~2íšŒ íˆ´ ì„ íƒ/ê´€ì°°/í›„ì²˜ë¦¬ ---
def rag_node(state: ReActState, llm: LLM) -> ReActState:
    state["ctx"] = retrieve_ctx(state["question"])  # ê²°ì •ì 
    steps = 0
    obs: Dict[str, Any] = {}
    while steps < MAX_STEPS:
        steps += 1
        ctx_snips = [c["text"] for c in state["ctx"]]
        tool_name = llm.choose_tool(state["question"], ctx_snips)
        if tool_name not in ALLOWED_TOOLS:
            state["trace"].append(f"Step{steps}: invalid_tool={tool_name}")
            break
        state["trace"].append(f"Step{steps}: Action={tool_name}")
        # Observation
        obs = ALLOWED_TOOLS[tool_name](state["question"])
        state["trace"].append(f"Step{steps}: Observation keys={list(obs.keys())}")
        # ê°„ë‹¨ groundedness ìŠ¤ì½”ì–´(ë°ëª¨)
        state["grounded"] = 0.6 if tool_name == Tool.SEARCH.value else 0.8
        # ê°„ë‹¨ í›„ì²˜ë¦¬: ë‹µ ìƒì„±
        draft = f"ì§ˆë¬¸: {state['question']}\në„êµ¬: {tool_name}\nê´€ì¸¡: {obs}\n"
        state["answer"] = llm.polish(draft)
        state["citations"] = [c["source"] for c in state["ctx"][:2]]
        break  # lite: í•œ ë²ˆì´ë©´ ì¶©ë¶„
    return state

# --- Tool-only ë…¸ë“œ (ì§ˆì˜ê°€ ëª…í™•í•˜ê²Œ íˆ´ ì§€ì¹­) ---
def tool_node(state: ReActState, llm: LLM) -> ReActState:
    tool_name = llm.choose_tool(state["question"], [])
    if tool_name not in ALLOWED_TOOLS:
        state["answer"] = "í—ˆìš©ë˜ì§€ ì•Šì€ ë„êµ¬ ìš”ì²­ì…ë‹ˆë‹¤."
        return state
    obs = ALLOWED_TOOLS[tool_name](state["question"])  # ê²°ì •ì 
    state["grounded"] = 0.85
    draft = f"ìš”ì²­ì„ '{tool_name}' ë„êµ¬ë¡œ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤. ê²°ê³¼ ìš”ì•½: {obs}"
    state["answer"] = llm.polish(draft)
    return state

# --- Direct ë…¸ë“œ (LLMë§Œ ìš”ì•½/ë‹µë³€) ---
def direct_node(state: ReActState, llm: LLM) -> ReActState:
    draft = f"ì§ˆë¬¸: {state['question']}\nì§ì ‘ ë‹µë³€ ì´ˆì•ˆ(ë°ëª¨): ê´€ë ¨ ì§€ì‹ì„ ìš”ì•½í•´ ì‘ë‹µí•©ë‹ˆë‹¤."
    state["answer"] = llm.polish(draft)
    state["grounded"] = 0.4
    return state

# --- Orchestrator (LangGraph ì‚¬ìš©) ---
def run_react_graph(question: str) -> Dict[str, Any]:
    llm = LLM()
    init: ReActState = {
        "question": question,
        "route": "direct",
        "ctx": [],
        "answer": "",
        "citations": [],
        "tries": 0,
        "grounded": 0.0,
        "trace": [],
    }

    if _LG:
        sg = StateGraph(ReActState)
        # ë…¸ë“œ ë˜í¼ (LangGraph ë…¸ë“œëŠ” stateë§Œ ë°›ìŒ)
        def _router(s: ReActState):
            return router(s)
        def _rag(s: ReActState):
            return rag_node(s, llm)
        def _tool(s: ReActState):
            return tool_node(s, llm)
        def _direct(s: ReActState):
            return direct_node(s, llm)

        sg.add_node("router", _router)
        sg.add_node("rag", _rag)
        sg.add_node("tool", _tool)
        sg.add_node("direct", _direct)
        sg.set_entry_point("router")
        sg.add_conditional_edges(
            "router",
            lambda s: s["route"],
            {"rag": "rag", "tool": "tool", "direct": "direct"},
        )
        sg.add_edge("rag", END)
        sg.add_edge("tool", END)
        sg.add_edge("direct", END)
        app = sg.compile()
        out = app.invoke(init)
        return out
    else:
        # LangGraph ë¯¸ì‚¬ìš© ê°„ì´ ì‹¤í–‰(ë™ì¼ ë¡œì§)
        st = router(init)
        if st["route"] == "rag":
            st = rag_node(st, llm)
        elif st["route"] == "tool":
            st = tool_node(st, llm)
        else:
            st = direct_node(st, llm)
        return st

# =====( 2) ìˆœìˆ˜ FSM(Graph-only) â€” ê²°ì •ì  í”Œë¡œìš° )=====
@dataclass
class OrderPayload:
    user_id: int
    city: str
    item: str

@dataclass
class FsmState:
    payload: OrderPayload
    valid: bool = False
    normalized: Dict[str, Any] = None
    api_result: Dict[str, Any] = None
    message: str = ""

# ê²°ì •ì  ë‹¨ê³„ë“¤

def fsm_validate(st: FsmState) -> FsmState:
    p = st.payload
    st.valid = bool(p.user_id and p.city and p.item)
    if not st.valid:
        st.message = "í•„ìˆ˜ ì…ë ¥ ëˆ„ë½"
    return st

def fsm_normalize(st: FsmState) -> FsmState:
    if not st.valid:
        return st
    st.normalized = {
        "user_id": int(st.payload.user_id),
        "city": st.payload.city.strip().title(),
        "item": st.payload.item.strip().lower(),
    }
    return st

def fsm_call_api(st: FsmState) -> FsmState:
    if not st.valid:
        return st
    # ê²°ì •ì  ì™¸ë¶€ í˜¸ì¶œ(ë°ëª¨: ë‚ ì”¨ + DB)
    w = tool_weather(st.normalized["city"])  # ê²°ì •ì 
    d = tool_db("SELECT 1")
    st.api_result = {"weather": w, "db": d}
    return st

def fsm_render(st: FsmState, llm: Optional[LLM] = None) -> FsmState:
    if not st.valid:
        return st
    draft = (
        f"ì£¼ë¬¸ ì ‘ìˆ˜: ì‚¬ìš©ì#{st.normalized['user_id']} / ë„ì‹œ={st.normalized['city']} / í’ˆëª©={st.normalized['item']}\n"
        f"í˜„ì¬ ë‚ ì”¨: {st.api_result['weather']['status']} {st.api_result['weather']['temp_c']}â„ƒ\n"
        f"ì²˜ë¦¬ ê²°ê³¼: OK"
    )
    st.message = llm.polish(draft) if llm else draft
    return st

# ê°„ì´ FSM ëŸ¬ë„ˆ (ê²°ì •ì )
def run_fsm_example(user_id: int = 1, city: str = "seoul", item: str = "umbrella") -> Dict[str, Any]:
    llm = LLM()
    st = FsmState(payload=OrderPayload(user_id=user_id, city=city, item=item))
    st = fsm_validate(st)
    st = fsm_normalize(st)
    st = fsm_call_api(st)
    st = fsm_render(st, llm)
    return {
        "valid": st.valid,
        "normalized": st.normalized,
        "api_result": st.api_result,
        "message": st.message,
    }

# =====( CLI )=====
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--mode", choices=["react", "fsm"], default="react")
    parser.add_argument("--q", default="ì„œìš¸ ë‚´ì¼ ë‚ ì”¨ ì•Œë ¤ì¤˜.")
    args = parser.parse_args()

    if args.mode == "react":
        out = run_react_graph(args.q)
        print("=== Constrained ReAct + Graph ===")
        print("route:", out.get("route"))
        print("answer:\n", out.get("answer"))
        print("citations:", out.get("citations"))
        print("grounded:", out.get("grounded"))
        print("trace:", out.get("trace"))
    else:
        out = run_fsm_example()
        print("=== FSM (Graph-only, ê²°ì •ì ) ===")
        for k, v in out.items():
            print(f"{k}: {v}")
